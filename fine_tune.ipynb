{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline, Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV dataset\n",
    "# Can't specify split because the dataset has not been divided to train and test yet\n",
    "# So everything is loaded to \"train\"\n",
    "dataset = load_dataset('csv', data_files='./datasets/youtoxic_english_1000.csv')\n",
    "\n",
    "# Define a function to do some preprocessing - rename target and map values\n",
    "# I'll rename the target column \"IsToxic\" to \"labels\".\n",
    "def preprocess_data(data):\n",
    "    # Rename columns    \n",
    "    data['labels'] = data.pop('IsToxic')\n",
    "    data['text'] = data.pop('Text')\n",
    "    # Map true/false to 1/0\n",
    "    data['labels'] = 1 if data['labels'] == True else 0\n",
    "    return data\n",
    "\n",
    "# Run the preprocessing\n",
    "dict_dataset = dataset.map(preprocess_data)\n",
    "\n",
    "# Split the entire dataset into train-test\n",
    "dict_train_test_split = dict_dataset[\"train\"].train_test_split(test_size=0.2, seed=823)\n",
    "\n",
    "# Further split the train to train-val\n",
    "dict_train_val_split = dict_train_test_split[\"train\"].train_test_split(test_size=0.3, seed=823)\n",
    "\n",
    "# Access the train and test sets\n",
    "dict_train = dict_train_val_split[\"train\"]\n",
    "dict_val = dict_train_val_split[\"test\"]\n",
    "dict_test = dict_train_test_split[\"test\"]\n",
    "\n",
    "# Define the tokenization function\n",
    "# Here is where you specify the input column\n",
    "def tokenize_function(input_string, tokenizer):\n",
    "    return tokenizer(input_string['text'], padding='max_length', truncation=True, truncation_strategy='longest_first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Fine-Tuning a Pre-trained Model (originally trained for a different task) using the custom data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize the Tokenizer and the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# ^Here, the AutoModelForSequenceClassification automatically adds a classification layer on top of the pre-trained BERT/DistilBERT model,\n",
    "# making it suitable for sequence classification tasks. \n",
    "# The underlying transformer model (DistilBERT in this case) serves as a feature extractor and the classification head predicts the class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 2161.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "dict_train_tokenized = dict_train.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "dict_test_tokenized = dict_test.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "dict_val_tokenized = dict_val.map(lambda x: tokenize_function(x, tokenizer), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup Hyperparms & Training Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\djbac\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Hyperparams + other config\n",
    "num_train_epochs = 5\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 16\n",
    "weight_decay = 0.01\n",
    "learning_rate = 2e-5\n",
    "logging_steps = 10\n",
    "warmup_steps = 20 # Calculated using total_train_steps = 0.1 * len(dict_train_tokenized) // per_device_train_batch_size*num_train_epochs\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/run1',                               # Directory to save results\n",
    "    num_train_epochs=num_train_epochs,                         # Number of training epochs\n",
    "    per_device_train_batch_size=per_device_train_batch_size,   # Batch size per device during training\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,     # Batch size per device during evaluation\n",
    "    warmup_steps=warmup_steps,                                 # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=weight_decay,                                 # Strength of weight decay\n",
    "    learning_rate=learning_rate,                               # Learning Rate\n",
    "    logging_dir='./logs',                                      # Directory for storing logs\n",
    "    logging_steps=logging_steps,                               # Frequency of logging steps\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",                                  \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=1 \n",
    "    # When save_total_limit=1 and load_best_model_at_end=True, \n",
    "    # it is possible that two checkpoints are saved: the last one and the best one (if they are different).\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                          # The model to train\n",
    "    args=training_args,                   # Training arguments\n",
    "    train_dataset=dict_train_tokenized,   # The dataset to train on\n",
    "    eval_dataset=dict_val_tokenized,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=97d9333c192a4f2081e63ef4707a8eeb\n",
      "2024-08-25 16:34:41,930 - clearml.Repository Detection - WARNING - Could not read Jupyter Notebook: No module named 'nbconvert'\n",
      "2024-08-25 16:34:41,999 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/a280739eaca04ea6b7be4b98665e026b/experiments/97d9333c192a4f2081e63ef4707a8eeb/output/log\n",
      "2024-08-25 16:34:50,260 - clearml.Task - WARNING - Parameters must be of builtin type (Transformers/accelerator_config[AcceleratorConfig])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–Ž         | 10/400 [00:13<07:52,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6935, 'grad_norm': 0.9224388599395752, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 20/400 [00:25<07:25,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6929, 'grad_norm': 2.55173921585083, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 30/400 [00:37<07:11,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6818, 'grad_norm': 1.7722618579864502, 'learning_rate': 1.2e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–‰         | 39/400 [00:47<07:01,  1.17s/it]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Using a Pre-trained Model directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"martin-ha/toxic-comment-model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model)  # Adjust num_labels as needed\n",
    "classifier = pipeline(task=\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Use pipeline(task=\"text-classification\") for simplicity and ease of use, especially for standard tasks where you don't need much customization.\n",
    "# Use TextClassificationPipeline when you need more control over the pipeline components or when you're working on a more complex or customized task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test = dict_test['text']\n",
    "actual_labels = dict_test[\"labels\"]\n",
    "# Make predictions\n",
    "predictions = classifier(inputs_test)\n",
    "# Predicted labels are \"non-toxic\" and \"toxic\", so we still need to map that\n",
    "label_mapping = {\"non-toxic\": 0, \"toxic\": 1}\n",
    "predicted_labels = [label_mapping[pred['label']] for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.85      0.78       118\n",
      "           1       0.70      0.51      0.59        82\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.71      0.68      0.68       200\n",
      "weighted avg       0.71      0.71      0.70       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report\n",
    "print(classification_report(actual_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### (3) Fine-Tuning using the on Custom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "# Here is where you specify the input column\n",
    "def tokenize_function(input_string):\n",
    "    return tokenizer(input_string['text'], padding='max_length', truncation=True, truncation_strategy='longest_first')\n",
    "\n",
    "# Apply the tokenization function to your dataset\n",
    "dict_train_tokenized = dict_train.map(tokenize_function, batched=True)\n",
    "dict_test_tokenized = dict_test.map(tokenize_function, batched=True)\n",
    "dict_val_tokenized = dict_val.map(tokenize_function, batched=True)\n",
    "\n",
    "# Format dataset for PyTorch\n",
    "# Here is where you specify the label/target column\n",
    "dict_train_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "dict_test_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "dict_val_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams + other config\n",
    "num_train_epochs = 10\n",
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 32\n",
    "weight_decay = 0.01\n",
    "learning_rate = 2e-5\n",
    "logging_steps = 10\n",
    "warmup_steps = 50 # Calculated using total_train_steps = 0.1 * len(dict_train_tokenized) // per_device_train_batch_size*num_train_epochs\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                                    # Directory to save results\n",
    "    num_train_epochs=num_train_epochs,                         # Number of training epochs\n",
    "    per_device_train_batch_size=per_device_train_batch_size,   # Batch size per device during training\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,     # Batch size per device during evaluation\n",
    "    warmup_steps=warmup_steps,                                 # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=weight_decay,                                 # Strength of weight decay\n",
    "    learning_rate=learning_rate,                               # Learning Rate\n",
    "    logging_dir='./logs',                                      # Directory for storing logs\n",
    "    logging_steps=logging_steps,                               # Frequency of logging steps\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",                                  \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=1 \n",
    "    # When save_total_limit=1 and load_best_model_at_end=True, \n",
    "    # it is possible that two checkpoints are saved: the last one and the best one (if they are different).\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                          # The model to train\n",
    "    args=training_args,                   # Training arguments\n",
    "    train_dataset=dict_train_tokenized,   # The dataset to train on\n",
    "    eval_dataset=dict_val_tokenized,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=f2bd721c76c848919ebcf0b58ad36b89\n",
      "2024-08-20 13:22:03,716 - clearml.Repository Detection - WARNING - Could not read Jupyter Notebook: No module named 'nbconvert'\n",
      "2024-08-20 13:22:03,766 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/a280739eaca04ea6b7be4b98665e026b/experiments/f2bd721c76c848919ebcf0b58ad36b89/output/log\n",
      "2024-08-20 13:22:11,837 - clearml.Task - WARNING - Parameters must be of builtin type (Transformers/accelerator_config[AcceleratorConfig])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–Ž         | 10/400 [00:36<24:17,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7439, 'grad_norm': 12.64841365814209, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 20/400 [01:15<24:36,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8521, 'grad_norm': 7.488814353942871, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 30/400 [01:52<22:10,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8086, 'grad_norm': 299.7953186035156, 'learning_rate': 1.2e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 40/400 [02:28<21:11,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7156, 'grad_norm': 3.7490789890289307, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 10%|â–ˆ         | 40/400 [02:47<21:11,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6793511509895325, 'eval_runtime': 19.0435, 'eval_samples_per_second': 8.402, 'eval_steps_per_second': 0.263, 'epoch': 1.0}\n",
      "2024-08-20 13:25:27,485 - clearml.storage - INFO - Starting upload: C:\\Users\\djbac\\AppData\\Local\\Temp\\model_package.vz4lml_p.zip => https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-40.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–Ž        | 50/400 [03:41<23:08,  3.97s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6041, 'grad_norm': 12.563817977905273, 'learning_rate': 2e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 60/400 [04:17<19:09,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6205, 'grad_norm': 3.959160089492798, 'learning_rate': 1.942857142857143e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 70/400 [04:53<18:46,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6236, 'grad_norm': 5.251299858093262, 'learning_rate': 1.885714285714286e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 80/400 [05:31<20:07,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6487, 'grad_norm': 3.435767650604248, 'learning_rate': 1.8285714285714288e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 20%|â–ˆâ–ˆ        | 80/400 [05:51<20:07,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6117069125175476, 'eval_runtime': 20.5631, 'eval_samples_per_second': 7.781, 'eval_steps_per_second': 0.243, 'epoch': 2.0}\n",
      "2024-08-20 13:28:35,533 - clearml.storage - WARNING - Failed deleting object uploading_file (404): File uploading_file not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not remove checkpoint `checkpoint-40` after going over the `save_total_limit`. Error is: Could not remove model id=7451857fbf1e495eabedff7dad3743e9 weights file 'https://files.clear.ml/uploading_file': Could not remove model id=7451857fbf1e495eabedff7dad3743e9 weights file: https://files.clear.ml/uploading_file\n",
      " 22%|â–ˆâ–ˆâ–Ž       | 90/400 [06:49<22:01,  4.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7548, 'grad_norm': 2.6698801517486572, 'learning_rate': 1.7714285714285717e-05, 'epoch': 2.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 100/400 [07:25<18:29,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5406, 'grad_norm': 6.709475040435791, 'learning_rate': 1.7142857142857142e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 110/400 [08:04<18:13,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6495, 'grad_norm': 2.365192174911499, 'learning_rate': 1.6571428571428574e-05, 'epoch': 2.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 120/400 [08:39<16:12,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.613, 'grad_norm': 4.1874799728393555, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 120/400 [09:00<16:12,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6756014823913574, 'eval_runtime': 21.6122, 'eval_samples_per_second': 7.403, 'eval_steps_per_second': 0.231, 'epoch': 3.0}\n",
      "2024-08-20 13:31:40,381 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:32:10,394 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:32:40,396 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:33:10,411 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:33:40,421 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:34:10,423 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:34:40,436 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:35:10,445 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:35:40,451 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:36:10,458 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:36:40,460 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:37:10,466 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:37:40,475 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:38:10,490 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:38:40,503 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:39:10,516 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:39:40,520 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:40:10,528 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:40:40,530 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:41:10,540 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:41:40,547 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:42:10,562 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:42:40,577 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:43:10,586 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:43:40,589 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:44:10,604 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:44:40,619 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:45:10,635 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:45:40,643 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:46:10,652 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:46:40,658 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:47:10,674 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:47:40,686 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:48:10,689 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:48:40,700 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:49:10,714 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:49:40,726 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:50:10,732 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:50:40,741 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:51:10,751 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:51:40,759 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:52:10,770 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:52:40,782 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:53:10,786 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:53:40,799 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:54:10,810 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:54:40,817 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:55:10,829 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:55:40,830 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:56:10,833 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:56:40,847 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:57:10,862 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:57:40,870 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:58:10,879 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:58:40,883 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:59:10,899 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 13:59:40,903 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:00:10,917 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:00:40,920 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:01:10,928 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:01:40,932 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:02:10,937 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:02:40,939 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:03:10,950 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:03:40,967 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:04:10,979 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:04:40,988 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:05:10,993 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:05:41,004 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:06:11,007 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:06:41,024 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:07:11,031 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:07:41,039 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:08:11,050 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:08:41,062 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:09:11,070 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:09:41,083 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-120.zip)\n",
      "2024-08-20 14:10:05,031 - clearml.Task - INFO - Completed model upload to https://files.clear.ml/HuggingFace%20Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-40.zip\n",
      "2024-08-20 14:10:06,182 - clearml.Task - ERROR - Action failed <400/201: models.edit/v1.0 (Invalid model id: company=ba9b1e32450c4261bb4aab543465389a, id=7451857fbf1e495eabedff7dad3743e9)> (model=7451857fbf1e495eabedff7dad3743e9, uri=https://files.clear.ml/HuggingFace%20Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-40.zip, name=checkpoint-40, comment=snapshot ['results\\\\checkpoint-40\\\\config.json', 'results\\\\checkpoint-40\\\\model.safetensors', 'results\\\\checkpoint-40\\\\optimizer.pt', 'results\\\\checkpoint-40\\\\rng_state.pth', 'results\\\\checkpoint-40\\\\scheduler.pt', 'results\\\\checkpoint-40\\\\trainer_state.json', 'results\\\\checkpoint-40\\\\training_args.bin'] stored\n",
      "Created by task id: f2bd721c76c848919ebcf0b58ad36b89, tags=[], system_tags=['package'], framework=TensorFlow, design={'design': ''}, labels={}, project=a280739eaca04ea6b7be4b98665e026b, task=f2bd721c76c848919ebcf0b58ad36b89, iteration=40)\n",
      "2024-08-20 14:10:06,183 - clearml.storage - WARNING - Exception on upload callback: Action failed <400/201: models.edit/v1.0 (Invalid model id: company=ba9b1e32450c4261bb4aab543465389a, id=7451857fbf1e495eabedff7dad3743e9)> (model=7451857fbf1e495eabedff7dad3743e9, uri=https://files.clear.ml/HuggingFace%20Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-40.zip, name=checkpoint-40, comment=snapshot ['results\\\\checkpoint-40\\\\config.json', 'results\\\\checkpoint-40\\\\model.safetensors', 'results\\\\checkpoint-40\\\\optimizer.pt', 'results\\\\checkpoint-40\\\\rng_state.pth', 'results\\\\checkpoint-40\\\\scheduler.pt', 'results\\\\checkpoint-40\\\\trainer_state.json', 'results\\\\checkpoint-40\\\\training_args.bin'] stored\n",
      "Created by task id: f2bd721c76c848919ebcf0b58ad36b89, tags=[], system_tags=['package'], framework=TensorFlow, design={'design': ''}, labels={}, project=a280739eaca04ea6b7be4b98665e026b, task=f2bd721c76c848919ebcf0b58ad36b89, iteration=40)\n",
      "2024-08-20 14:10:06,185 - clearml.storage - INFO - Starting upload: C:\\Users\\djbac\\AppData\\Local\\Temp\\model_package.e6fr6svl.zip => https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-80.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not remove checkpoint `checkpoint-40` after going over the `save_total_limit`. Error is: Could not remove model id=7451857fbf1e495eabedff7dad3743e9: <400/201: models.delete/v1.0 (Invalid model id: company=ba9b1e32450c4261bb4aab543465389a, id=7451857fbf1e495eabedff7dad3743e9)>\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 130/400 [48:26<2:24:38, 32.14s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6894, 'grad_norm': 5.3642706871032715, 'learning_rate': 1.542857142857143e-05, 'epoch': 3.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 140/400 [49:03<19:28,  4.49s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.581, 'grad_norm': 42.47323989868164, 'learning_rate': 1.4857142857142858e-05, 'epoch': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 150/400 [49:40<15:30,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7223, 'grad_norm': 17.145112991333008, 'learning_rate': 1.4285714285714287e-05, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 160/400 [50:17<14:46,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6418, 'grad_norm': 3.012005567550659, 'learning_rate': 1.3714285714285716e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 160/400 [50:35<14:46,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7141662240028381, 'eval_runtime': 17.8451, 'eval_samples_per_second': 8.966, 'eval_steps_per_second': 0.28, 'epoch': 4.0}\n",
      "2024-08-20 14:13:15,303 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-160.zip)\n",
      "2024-08-20 14:13:45,308 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-160.zip)\n",
      "2024-08-20 14:14:15,320 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-160.zip)\n",
      "2024-08-20 14:14:45,329 - clearml.Task - INFO - Waiting for previous model to upload (2 pending, https://files.clear.ml/HuggingFace Transformers/Trainer.f2bd721c76c848919ebcf0b58ad36b89/models/checkpoint-160.zip)\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model (optional jic you got disconnected to the kernel, here's how you load the model)\n",
    "model_path = './results'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.6198015809059143, 'eval_runtime': 47.3297, 'eval_samples_per_second': 4.226, 'eval_steps_per_second': 0.148, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "eval_results = trainer.evaluate(eval_dataset=dict_test_tokenized)\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:38<00:00,  5.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "predictions, labels, _ = trainer.predict(test_dataset=dict_test_tokenized)\n",
    "# Convert predictions to labels\n",
    "predicted_labels = predictions.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.66      0.74       118\n",
      "           1       0.63      0.82      0.71        82\n",
      "\n",
      "    accuracy                           0.72       200\n",
      "   macro avg       0.73      0.74      0.72       200\n",
      "weighted avg       0.75      0.72      0.73       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0], dtype=int64)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latest-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
